{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "import sys\n",
    "\n",
    "\n",
    "#hyper_parameter\n",
    "C = 1\n",
    "bigram_thresh = 7\n",
    "negation_thresh = 2\n",
    "punct_list = [\",\",\".\",\"/\",\"\\\"\"]\n",
    "negate_list = [\"not\",\"no\",\"never\",\"didn't\",\"nt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/saket/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/saket/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/saket/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initializing stemmer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "en_stop = set(stopwords.words('english'))\n",
    "p_stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(documents):\n",
    "\tnew_documents = []\n",
    "\tfor document in documents:\n",
    "\t\tfor i in range(len(punct_list)):\n",
    "\t\t\tdocument = document.lower()\n",
    "\t\t\tdocument = document.replace(punct_list[i],\" \")\n",
    "\t\tnew_documents.append(document)\n",
    "\treturn new_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStemmedDocument(input_raw_data,adjective,negation_token):\n",
    "    docs = input_raw_data\n",
    "    new_doc = []\n",
    "    for doc in docs:\n",
    "        doc = doc.decode('utf-8')        \n",
    "        raw = doc.lower()\n",
    "        raw = raw.replace(\"<br /><br />\", \" \")\n",
    "        raw.replace(\" br \",\" \")\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        if adjective>0 :\n",
    "            pos = nltk.pos_tag(tokens)\n",
    "            adj_list = [tag[0] for tag in pos if tag[1] == 'JJ']\n",
    "        stopped_tokens = [token for token in tokens if token not in en_stop]\n",
    "        stemmed_tokens = [p_stemmer.stem(token) for token in stopped_tokens]\n",
    "        if adjective>0 :\n",
    "            stemmed_adj_tokens = [p_stemmer.stem(token) for token in adj_list]\n",
    "            for i in range(adjective):\n",
    "                stemmed_tokens = stemmed_tokens + stemmed_adj_tokens\n",
    "        if negation_token:\n",
    "            stemmed_tokens = not_clear(stemmed_tokens)\n",
    "        documentWords = ' '.join(stemmed_tokens)\n",
    "        new_doc.append(documentWords)\n",
    "    return new_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_clear(tokens):\n",
    "    i =0\n",
    "    for token in tokens:\n",
    "        if token in negate_list:\n",
    "            if i+1 < len(tokens):\n",
    "                tokens[i+1] = \"not\" + tokens[i+1]\n",
    "            if i+2 < len(tokens):\n",
    "                tokens[i+2] = \"not\" + tokens[i+2]\n",
    "        i+=1\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "\tfile = open(\"data/\" + file_name)\n",
    "\tall_text = file.readlines()\n",
    "\tif len(all_text)==0:\n",
    "\t\tprint \"empty document\"\n",
    "\treturn all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dict(training_data,bigram):\n",
    "\tvocab_dict = {}\n",
    "\tbigram_dict = {}\n",
    "\tcount = 0   \n",
    "\tfor document in training_data:\n",
    "\t\twords = document.split()\n",
    "\t\ti = 0\n",
    "\t\tfor word in words:\n",
    "\t\t\tif i > 0 and bigram:\n",
    "\t\t\t\tbigram_word = words[i-1] + \" \" + word\n",
    "\t\t\t\tif not bigram_word in bigram_dict:\n",
    "\t\t\t\t\tbigram_dict[bigram_word] = 1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tbigram_dict[bigram_word] +=1\n",
    "\t\t\tif not word in vocab_dict:\n",
    "\t\t\t\tvocab_dict[word] = count\n",
    "\t\t\t\tcount += 1\n",
    "\t\t\ti +=1\n",
    "\tif bigram:\n",
    "\t\tfor key in bigram_dict.keys():\n",
    "\t\t\tif bigram_dict[key] >= bigram_thresh:              \n",
    "\t\t\t\tvocab_dict[key] = count\n",
    "\t\t\t\tcount += 1\n",
    "\treturn vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_document,bigram):\n",
    "\tglobal naive_matrix,label_freq,number_classes,vocab_dict\n",
    "\tmax_sum = 0\n",
    "\tpredicted_class = -1\n",
    "\tfor class_ in range(number_classes):\n",
    "\t\tsums = math.log(label_freq[class_])\n",
    "\t\twords = test_document.split()\n",
    "\t\tj = 0\n",
    "\t\tfor word in words:\n",
    "\t\t\tif j>0 and bigram:\n",
    "\t\t\t\tbigram_word = words[j-1] + \" \" + word\n",
    "\t\t\t\tif bigram_word in vocab_dict:\n",
    "\t\t\t\t\tword_index = vocab_dict[bigram_word]\n",
    "\t\t\t\t\tsums += naive_matrix[word_index][class_]\n",
    "\t\t\tif word in vocab_dict:\n",
    "\t\t\t\tword_index = vocab_dict[word]\n",
    "\t\t\t\tsums += naive_matrix[word_index][class_]\n",
    "\t\t\tj += 1\n",
    "\t\tif sums > max_sum or class_==0:\n",
    "\t\t\tmax_sum = sums\n",
    "\t\t\tpredicted_class = class_\n",
    "\treturn predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_label(label_val):\n",
    "    for key in label_dict.keys():\n",
    "        if label_dict[key] == label_val:\n",
    "            return key\n",
    "def indices(label_key):\n",
    "    label_key = int(label_key)\n",
    "    if label_key >= 7:\n",
    "        label_key -= 2\n",
    "    return label_key - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(test_documents,labels,bigram):\n",
    "\tcorrect = 0.0\n",
    "\tfor i in range(len(test_documents)):\n",
    "\t\tpredicted_class = predict(test_documents[i],bigram)\n",
    "\t\tif labels[i].split()[0] in label_dict.keys():\n",
    "\t\t\texpected_class = label_dict[labels[i].split()[0]]\n",
    "\t\t\tif predicted_class==expected_class:\n",
    "\t\t\t\tcorrect+=1\n",
    "\treturn correct/len(test_documents)\n",
    "\n",
    "def majority_accuracy(test_documents,labels):\n",
    "\tcorrect = 0.0\n",
    "\tfor i in range(len(test_documents)):\n",
    "\t\tpredicted_class = np.argmax(label_freq)\n",
    "\t\tif labels[i].split()[0] in label_dict.keys():\n",
    "\t\t\texpected_class = label_dict[labels[i].split()[0]]\n",
    "\t\t\tif predicted_class==expected_class:\n",
    "\t\t\t\tcorrect+=1\n",
    "\treturn correct/len(test_documents)\n",
    "\n",
    "def random_accuracy(test_documents,labels):\n",
    "\tcorrect = 0.0\n",
    "\tfor i in range(len(test_documents)):\n",
    "\t\tpredicted_class = random.randint(0,len(label_dict))\n",
    "\t\tif labels[i].split()[0] in label_dict:\n",
    "\t\t\texpected_class = label_dict[labels[i].split()[0]]\n",
    "\t\t\tif predicted_class==expected_class:\n",
    "\t\t\t\tcorrect+=1\n",
    "\treturn correct/len(test_documents)\n",
    "\n",
    "def confusion_matrix(test_documents,labels,bigram):\n",
    "\tcorrect = np.zeros((len(label_dict),len(label_dict)))\n",
    "\tfor i in range(len(test_documents)):\n",
    "\t\tpredicted_class = predict(test_documents[i],bigram)\n",
    "\t\tpredicted_class_key = inv_label(predicted_class)\n",
    "\t\tif labels[i].split()[0] in label_dict:\n",
    "\t\t\texpected_class_key = labels[i].split()[0]\n",
    "\t\t\tcorrect[indices(expected_class_key)][indices(predicted_class_key)] += 1\n",
    "\treturn (correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf_fill(training_data):\n",
    "    idf_count = np.zeros(len(vocab_dict))\n",
    "    for doc in training_data:\n",
    "        words = doc.split()\n",
    "        word_set = set()\n",
    "        for word in words:\n",
    "            word_set.add(word)\n",
    "        for word in word_set:\n",
    "            word_index = vocab_dict[word]\n",
    "            idf_count[word_index] += 1\n",
    "    print idf_count\n",
    "    idf_count =  np.log(len(training_data)) - np.log(idf_count)\n",
    "    return idf_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_matrix(bigram,idf):\n",
    "\tnaive_matrix = np.ones((len(vocab_dict),number_classes))\n",
    "\tnaive_matrix = C * naive_matrix    \n",
    "\tnum_words_in_class = np.full((1,number_classes),C*len(vocab_dict))\n",
    "\tlabel_freq = np.zeros(number_classes)\n",
    "\tfor i in range(len(training_data)):\n",
    "\t\tlabel = label_dict[training_labels[i].split()[0]]\n",
    "\t\tlabel_freq[label] +=1\n",
    "\t\twords = training_data[i].split()\n",
    "\t\tj = 0\n",
    "\t\tfor word in words:\n",
    "\t\t\tif j>0 and bigram:\n",
    "\t\t\t\tbigram_word = words[j-1] + \" \" + word\n",
    "\t\t\t\tif bigram_word in vocab_dict:\n",
    "\t\t\t\t\tword_index = vocab_dict[bigram_word]\n",
    "\t\t\t\t\tnaive_matrix[word_index][label] +=1\n",
    "\t\t\tword_index = vocab_dict[word]\n",
    "\t\t\tnaive_matrix[word_index][label] += (1 + idf*idf_count[word_index])\n",
    "\t\t\tnum_words_in_class[0][label] += (1 + idf*idf_count[word_index])\n",
    "\t\t\tj +=1\n",
    "\treturn (np.log(naive_matrix) - np.log(num_words_in_class)),label_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(feature):\n",
    "    training_data = read_data(\"imdb_train_text.txt\")\n",
    "    if feature==2 or feature ==3 or feature ==5:\n",
    "        training_data = getStemmedDocument(training_data,0,0)\n",
    "    elif feature==4:\n",
    "        training_data = getStemmedDocument(training_data,0,1)\n",
    "    training_labels = read_data(\"imdb_train_labels.txt\")\n",
    "    test_data = read_data(\"imdb_test_text.txt\")\n",
    "    if feature==2 or feature ==3 or feature ==5:\n",
    "        test_data = getStemmedDocument(test_data,0,0)\n",
    "    elif feature ==4:\n",
    "        test_data = getStemmedDocument(test_data,0,1)\n",
    "    test_labels = read_data(\"imdb_test_labels.txt\")\n",
    "    if feature == 3:\n",
    "        bigram = 1\n",
    "    else:\n",
    "        bigram = 0\n",
    "    label_dict = make_dict(training_labels,0)\n",
    "    vocab_dict = make_dict(training_data,bigram)\n",
    "    number_classes = len(label_dict)\n",
    "    if feature ==5:\n",
    "        idf_count  = idf_fill(training_data)\n",
    "        naive_matrix,label_freq = make_matrix(bigram,1)\n",
    "    else:\n",
    "        naive_matrix,label_freq = make_matrix(bigram,0)\n",
    "    training_accuracy = accuracy(training_data,training_labels,bigram)\n",
    "    print training_accuracy*100\n",
    "    test_accuracy = accuracy(test_data,test_labels,bigram)\n",
    "    print test_accuracy*100\n",
    "    confuse = confusion_matrix(test_data,test_labels,bigram)\n",
    "    for i in range(len(confuse)):\n",
    "        for j in range(len(confuse[0])):\n",
    "            print (int)(confuse[i][j]),\n",
    "        print\n",
    "    print \"================================================================================\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal\n",
      "[[-12.54657603 -12.54657603 -12.54657603 ..., -12.54657603 -12.54657603\n",
      "  -12.54657603]\n",
      " [-12.54657603 -12.54657603 -12.54657603 ..., -12.54657603 -12.54657603\n",
      "  -12.54657603]\n",
      " [-12.54657603 -12.54657603 -12.54657603 ..., -12.54657603 -12.54657603\n",
      "  -12.54657603]\n",
      " ..., \n",
      " [-12.54657603 -12.54657603 -12.54657603 ..., -12.54657603 -12.54657603\n",
      "  -12.54657603]\n",
      " [-12.54657603 -12.54657603 -12.54657603 ..., -12.54657603 -12.54657603\n",
      "  -12.54657603]\n",
      " [-12.54657603 -12.54657603 -12.54657603 ..., -12.54657603 -12.54657603\n",
      "  -12.54657603]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-3b7d327e5e2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"normal\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mfeature_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"stemmed\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mfeature_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-74-db72defde957>\u001b[0m in \u001b[0;36mfeature_selection\u001b[0;34m(feature)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mnaive_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mnaive_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtraining_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnaive_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mtraining_accuracy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnaive_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-8642abbee432>\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(naive_matrix, label_freq, label_dict, vocab_dict, test_documents, labels, bigram)\u001b[0m\n\u001b[1;32m      2\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                 \u001b[0mpredicted_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnaive_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_documents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                         \u001b[0mexpected_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-78f7a6ba2cfc>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(naive_matrix, label_freq, label_dict, vocab_dict, test_document, bigram)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mpredicted_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mclass_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                 \u001b[0msums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_freq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                 \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_document\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "vocab_dict = {}\n",
    "training_data = []\n",
    "number_classes = 0\n",
    "label_dict = {}\n",
    "\n",
    "print \"normal\"\n",
    "feature_selection(1)\n",
    "print \"stemmed\"\n",
    "feature_selection(2)\n",
    "print \"bigram\"\n",
    "feature_selection(3)\n",
    "print \"negation\"\n",
    "feature_selection(4)\n",
    "print \"idf\"\n",
    "feature_selection(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
