{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "import sys\n",
    "\n",
    "\n",
    "#hyper_parameter\n",
    "C = 1\n",
    "bigram_thresh = 7\n",
    "negation_thresh = 2\n",
    "punct_list = [\",\",\".\",\"/\",\"\\\"\"]\n",
    "negate_list = [\"not\",\"no\",\"never\",\"didn't\",\"nt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/saket/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/saket/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/saket/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initializing stemmer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "en_stop = set(stopwords.words('english'))\n",
    "p_stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(documents):\n",
    "\tnew_documents = []\n",
    "\tfor document in documents:\n",
    "\t\tfor i in range(len(punct_list)):\n",
    "\t\t\tdocument = document.lower()\n",
    "\t\t\tdocument = document.replace(punct_list[i],\" \")\n",
    "\t\tnew_documents.append(document)\n",
    "\treturn new_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStemmedDocument(input_raw_data,adjective):\n",
    "    docs = input_raw_data\n",
    "    new_doc = []\n",
    "    for doc in docs:\n",
    "        doc = doc.decode('utf-8')        \n",
    "        raw = doc.lower()\n",
    "        raw = raw.replace(\"<br /><br />\", \" \")\n",
    "        raw.replace(\" br \",\" \")\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        if adjective>0 :\n",
    "            pos = nltk.pos_tag(tokens)\n",
    "            adj_list = [tag[0] for tag in pos if tag[1] == 'JJ']\n",
    "        stopped_tokens = [token for token in tokens if token not in en_stop]\n",
    "        stemmed_tokens = [p_stemmer.stem(token) for token in stopped_tokens]\n",
    "        if adjective>0 :\n",
    "            stemmed_adj_tokens = [p_stemmer.stem(token) for token in adj_list]\n",
    "            for i in range(adjective):\n",
    "                stemmed_tokens = stemmed_tokens + stemmed_adj_tokens\n",
    "        stemmed_tokens = not_clear(stemmed_tokens)\n",
    "        documentWords = ' '.join(stemmed_tokens)\n",
    "        new_doc.append(documentWords)\n",
    "    return new_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_clear(tokens):\n",
    "    i =0\n",
    "    for token in tokens:\n",
    "        if token in negate_list:\n",
    "            if i+1 < len(tokens):\n",
    "                tokens[i+1] = \"not\" + tokens[i+1]\n",
    "            if i+2 < len(tokens):\n",
    "                tokens[i+2] = \"not\" + tokens[i+2]\n",
    "        i+=1\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "\tfile = open(\"data/\" + file_name)\n",
    "\tall_text = file.readlines()\n",
    "\tif len(all_text)==0:\n",
    "\t\tprint \"empty document\"\n",
    "\treturn all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dict(training_data,bigram):\n",
    "\tvocab_dict = {}\n",
    "\tbigram_dict = {}\n",
    "\tcount = 0   \n",
    "\tfor document in training_data:\n",
    "\t\twords = document.split()\n",
    "\t\ti = 0\n",
    "\t\tfor word in words:\n",
    "\t\t\tif i > 0 and bigram:\n",
    "\t\t\t\tbigram_word = words[i-1] + \" \" + word\n",
    "\t\t\t\tif not bigram_word in bigram_dict:\n",
    "\t\t\t\t\tbigram_dict[bigram_word] = 1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tbigram_dict[bigram_word] +=1\n",
    "\t\t\tif not word in vocab_dict:\n",
    "\t\t\t\tvocab_dict[word] = count\n",
    "\t\t\t\tcount += 1\n",
    "\t\t\ti +=1\n",
    "\tif bigram:\n",
    "\t\tfor key in bigram_dict.keys():\n",
    "\t\t\tif bigram_dict[key] >= bigram_thresh:              \n",
    "\t\t\t\tvocab_dict[key] = count\n",
    "\t\t\t\tcount += 1\n",
    "\treturn vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_document,bigram):\n",
    "\tglobal naive_matrix,label_freq,number_classes,vocab_dict\n",
    "\tmax_sum = 0\n",
    "\tpredicted_class = -1\n",
    "\tfor class_ in range(number_classes):\n",
    "\t\tsums = math.log(label_freq[class_])\n",
    "\t\twords = test_document.split()\n",
    "\t\tj = 0\n",
    "\t\tfor word in words:\n",
    "\t\t\tif j>0 and bigram:\n",
    "\t\t\t\tbigram_word = words[j-1] + \" \" + word\n",
    "\t\t\t\tif bigram_word in vocab_dict:\n",
    "\t\t\t\t\tword_index = vocab_dict[bigram_word]\n",
    "\t\t\t\t\tsums += naive_matrix[word_index][class_]\n",
    "\t\t\tif word in vocab_dict:\n",
    "\t\t\t\tword_index = vocab_dict[word]\n",
    "\t\t\t\tsums += naive_matrix[word_index][class_]\n",
    "\t\t\tj += 1\n",
    "\t\tif sums > max_sum or class_==0:\n",
    "\t\t\tmax_sum = sums\n",
    "\t\t\tpredicted_class = class_\n",
    "\treturn predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_label(label_val):\n",
    "    for key in label_dict.keys():\n",
    "        if label_dict[key] == label_val:\n",
    "            return key\n",
    "def indices(label_key):\n",
    "    label_key = int(label_key)\n",
    "    if label_key >= 7:\n",
    "        label_key -= 2\n",
    "    return label_key - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(test_documents,labels,bigram):\n",
    "\tcorrect = 0.0\n",
    "\tfor i in range(len(test_documents)):\n",
    "\t\tpredicted_class = predict(test_documents[i],bigram)\n",
    "\t\tif labels[i].split()[0] in label_dict.keys():\n",
    "\t\t\texpected_class = label_dict[labels[i].split()[0]]\n",
    "\t\t\tif predicted_class==expected_class:\n",
    "\t\t\t\tcorrect+=1\n",
    "\treturn correct/len(test_documents)\n",
    "\n",
    "def majority_accuracy(test_documents,labels):\n",
    "\tcorrect = 0.0\n",
    "\tfor i in range(len(test_documents)):\n",
    "\t\tpredicted_class = np.argmax(label_freq)\n",
    "\t\tif labels[i].split()[0] in label_dict.keys():\n",
    "\t\t\texpected_class = label_dict[labels[i].split()[0]]\n",
    "\t\t\tif predicted_class==expected_class:\n",
    "\t\t\t\tcorrect+=1\n",
    "\treturn correct/len(test_documents)\n",
    "\n",
    "def random_accuracy(test_documents,labels):\n",
    "\tcorrect = 0.0\n",
    "\tfor i in range(len(test_documents)):\n",
    "\t\tpredicted_class = random.randint(0,len(label_dict))\n",
    "\t\tif labels[i].split()[0] in label_dict:\n",
    "\t\t\texpected_class = label_dict[labels[i].split()[0]]\n",
    "\t\t\tif predicted_class==expected_class:\n",
    "\t\t\t\tcorrect+=1\n",
    "\treturn correct/len(test_documents)\n",
    "\n",
    "def confusion_matrix(test_documents,labels,bigram):\n",
    "\tcorrect = np.zeros((len(label_dict),len(label_dict)))\n",
    "\tfor i in range(len(test_documents)):\n",
    "\t\tpredicted_class = predict(test_documents[i],bigram)\n",
    "\t\tpredicted_class_key = inv_label(predicted_class)\n",
    "\t\tif labels[i].split()[0] in label_dict:\n",
    "\t\t\texpected_class_key = labels[i].split()[0]\n",
    "\t\t\tcorrect[indices(expected_class_key)][indices(predicted_class_key)] += 1\n",
    "\treturn (correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 20000\n",
    "training_data = read_data(\"imdb_train_text.txt\")\n",
    "# training_data = training_data[0:sample_size]\n",
    "\n",
    "training_data = getStemmedDocument(training_data,0)\n",
    "# training_data = remove_punct(training_data)\n",
    "training_labels = read_data(\"imdb_train_labels.txt\")\n",
    "# training_labels = training_labels[0:sample_size]\n",
    "\n",
    "test_data = read_data(\"imdb_test_text.txt\")\n",
    "# test_data = test_data[0:sample_size]\n",
    "\n",
    "test_data = getStemmedDocument(test_data,0)\n",
    "# test_data = remove_punct(test_data)\n",
    "test_labels = read_data(\"imdb_test_labels.txt\")\n",
    "# test_labels = test_labels[0:sample_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94640\n"
     ]
    }
   ],
   "source": [
    "label_dict = make_dict(training_labels,0)\n",
    "vocab_dict = make_dict(training_data,1)\n",
    "number_classes = len(label_dict)\n",
    "print len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf_fill(training_data):\n",
    "    idf_count = np.zeros(len(vocab_dict))\n",
    "    for doc in training_data:\n",
    "        words = doc.split()\n",
    "        word_set = set()\n",
    "        for word in words:\n",
    "            word_set.add(word)\n",
    "        for word in word_set:\n",
    "            word_index = vocab_dict[word]\n",
    "            idf_count[word_index] += 1\n",
    "    print idf_count\n",
    "    idf_count =  np.log(len(training_data)) - np.log(idf_count)\n",
    "    return idf_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_matrix(bigram,idf):\n",
    "\tnaive_matrix = np.ones((len(vocab_dict),number_classes))\n",
    "\tnaive_matrix = C * naive_matrix    \n",
    "\tnum_words_in_class = np.full((1,number_classes),C*len(vocab_dict))\n",
    "\tlabel_freq = np.zeros(number_classes)\n",
    "\tfor i in range(len(training_data)):\n",
    "\t\tlabel = label_dict[training_labels[i].split()[0]]\n",
    "\t\tlabel_freq[label] +=1\n",
    "\t\twords = training_data[i].split()\n",
    "\t\tj = 0\n",
    "\t\tfor word in words:\n",
    "\t\t\tif j>0 and bigram:\n",
    "\t\t\t\tbigram_word = words[j-1] + \" \" + word\n",
    "\t\t\t\tif bigram_word in vocab_dict:\n",
    "\t\t\t\t\tword_index = vocab_dict[bigram_word]\n",
    "\t\t\t\t\tnaive_matrix[word_index][label] +=1\n",
    "\t\t\tword_index = vocab_dict[word]\n",
    "\t\t\tnaive_matrix[word_index][label] += (1 + idf*idf_count[word_index])\n",
    "\t\t\tnum_words_in_class[0][label] += (1 + idf*idf_count[word_index])\n",
    "\t\t\tj +=1\n",
    "\treturn (np.log(naive_matrix) - np.log(num_words_in_class)),label_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idf_count  = idf_fill(training_data)\n",
    "naive_matrix,label_freq = make_matrix(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.22\n"
     ]
    }
   ],
   "source": [
    "training_accuracy = accuracy(training_data,training_labels,1)\n",
    "print training_accuracy*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.252\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = accuracy(test_data,test_labels,1)\n",
    "print test_accuracy*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.204\n"
     ]
    }
   ],
   "source": [
    "test_random_accuracy = random_accuracy(test_data,test_labels)\n",
    "print test_random_accuracy*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.088\n"
     ]
    }
   ],
   "source": [
    "test_majority_accuracy = majority_accuracy(test_data,test_labels)\n",
    "print test_majority_accuracy*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "confuse = confusion_matrix(test_data,test_labels,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4261 72 151 247 41 70 24 156\n",
      "1574 48 164 282 62 45 9 118\n",
      "1349 64 229 497 107 110 16 169\n",
      "991 53 254 650 199 221 29 238\n",
      "364 19 87 286 374 522 79 576\n",
      "380 18 80 198 298 717 137 1022\n",
      "299 7 37 107 147 446 107 1194\n",
      "640 13 49 126 187 517 159 3308\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(confuse)):\n",
    "    for j in range(len(confuse[0])):\n",
    "        print (int)(confuse[i][j]),\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10': 0, '1': 7, '3': 4, '2': 6, '4': 5, '7': 2, '9': 3, '8': 1}\n"
     ]
    }
   ],
   "source": [
    "print label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
