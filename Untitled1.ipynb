{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import sys\n",
    "\n",
    "\n",
    "#hyper_parameter\n",
    "bigram_thresh = 3\n",
    "punct_list = [\",\",\".\",\"/\",\"\\\"\",\"'\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/saket/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing stemmer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "en_stop = set(stopwords.words('english'))\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(documents):\n",
    "\tnew_documents = []\n",
    "\tfor document in documents:\n",
    "\t\tfor i in range(len(punct_list)):\n",
    "\t\t\tdocument = document.lower()\n",
    "\t\t\tdocument = document.replace(punct_list[i],\" \")\n",
    "\t\tnew_documents.append(document)\n",
    "\treturn new_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStemmedDocument(input_raw_data):\n",
    "\tdocs = input_raw_data\n",
    "\tnew_doc = []\n",
    "\tfor doc in docs:\n",
    "\t\traw = doc.lower()\n",
    "\t\traw.decode('utf-8','ignore')\n",
    "\t\traw = raw.replace(\"<br /><br />\", \" \")\n",
    "\t\ttokens = tokenizer.tokenize(raw)\n",
    "\t\tstopped_tokens = [token for token in tokens if token not in en_stop]\n",
    "\t\tstemmed_tokens = [p_stemmer.stem(token) for token in stopped_tokens]\n",
    "\t\tprint stemmed_tokens\n",
    "\t\tdocumentWords = ' '.join(stemmed_tokens)\n",
    "\t\tnew_doc.append(documentWords)\n",
    "\treturn new_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "\tfile = open(\"data/\" + file_name)\n",
    "\tall_text = file.readlines()\n",
    "\tif len(all_text)==0:\n",
    "\t\tprint \"empty document\"\n",
    "\treturn all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dict(training_data,bigram):\n",
    "\tvocab_dict = {}\n",
    "\tbigram_dict = {}\n",
    "\tcount = 0   \n",
    "\tfor document in training_data:\n",
    "\t\twords = document.split()\n",
    "\t\ti = 0\n",
    "\t\tfor word in words:\n",
    "\t\t\tif i > 0 and bigram:\n",
    "\t\t\t\tbigram_word = words[i-1] + \" \" + word\n",
    "\t\t\t\tif not bigram_word in bigram_dict:\n",
    "\t\t\t\t\tbigram_dict[bigram_word] = 1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tbigram_dict[bigram_word] +=1\n",
    "\t\t\tif not word in vocab_dict:\n",
    "\t\t\t\tvocab_dict[word] = count\n",
    "\t\t\t\tcount += 1\n",
    "\t\t\ti +=1\n",
    "\tif bigram:\n",
    "\t\tfor key in bigram_dict.keys():\n",
    "\t\t\tif bigram_dict[key] >= bigram_thresh:\n",
    "\t\t\t\tvocab_dict[key] = count\n",
    "\t\t\t\tcount += 1\n",
    "\treturn vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_document,bigram):\n",
    "\tglobal naive_matrix,label_freq,number_classes,vocab_dict\n",
    "\tmax_sum = 0\n",
    "\tpredicted_class = -1\n",
    "\tfor class_ in range(number_classes):\n",
    "\t\tsums = math.log(label_freq[class_])\n",
    "\t\twords = test_document.split()\n",
    "\t\tj = 0\n",
    "\t\tfor word in words:\n",
    "\t\t\tif j>0 and bigram:\n",
    "\t\t\t\tbigram_word = words[j-1] + \" \" + word\n",
    "\t\t\t\tif bigram_word in vocab_dict:\n",
    "\t\t\t\t\tword_index = vocab_dict[bigram_word]\n",
    "\t\t\t\t\tsums += naive_matrix[word_index][class_]\n",
    "\t\t\tif word in vocab_dict:\n",
    "\t\t\t\tword_index = vocab_dict[word]\n",
    "\t\t\t\tsums += naive_matrix[word_index][class_]\n",
    "\t\t\tj += 1\n",
    "\t\tif sums > max_sum or class_==0:\n",
    "\t\t\tmax_sum = sums\n",
    "\t\t\tpredicted_class = class_\n",
    "\treturn predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_label(label_val):\n",
    "    for key in label_dict.keys():\n",
    "        if label_dict[key] == label_val:\n",
    "            return key\n",
    "def indices(label_key):\n",
    "    label_key = int(label_key)\n",
    "    if label_key >= 7:\n",
    "        label_key -= 2\n",
    "    return label_key - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(test_documents,labels,bigram):\n",
    "\tcorrect = 0.0\n",
    "\tfor i in range(len(test_documents)):\n",
    "\t\tpredicted_class = predict(test_documents[i],bigram)\n",
    "\t\tif labels[i].split()[0] in label_dict.keys():\n",
    "\t\t\texpected_class = label_dict[labels[i].split()[0]]\n",
    "\t\t\tif predicted_class==expected_class:\n",
    "\t\t\t\tcorrect+=1\n",
    "\treturn correct/len(test_documents)\n",
    "\n",
    "def majority_accuracy(test_documents,labels):\n",
    "\tcorrect = 0.0\n",
    "\tfor i in range(len(test_documents)):\n",
    "\t\tpredicted_class = np.argmax(label_freq)\n",
    "\t\tif labels[i].split()[0] in label_dict.keys():\n",
    "\t\t\texpected_class = label_dict[labels[i].split()[0]]\n",
    "\t\t\tif predicted_class==expected_class:\n",
    "\t\t\t\tcorrect+=1\n",
    "\treturn correct/len(test_documents)\n",
    "\n",
    "def random_accuracy(test_documents,labels):\n",
    "\tcorrect = 0.0\n",
    "\tfor i in range(len(test_documents)):\n",
    "\t\tpredicted_class = random.randint(0,len(label_dict))\n",
    "\t\tif labels[i].split()[0] in label_dict:\n",
    "\t\t\texpected_class = label_dict[labels[i].split()[0]]\n",
    "\t\t\tif predicted_class==expected_class:\n",
    "\t\t\t\tcorrect+=1\n",
    "\treturn correct/len(test_documents)\n",
    "\n",
    "def confusion_matrix(test_documents,labels,bigram):\n",
    "\tcorrect = np.zeros((len(label_dict),len(label_dict)))\n",
    "\tfor i in range(len(test_documents)):\n",
    "\t\tpredicted_class = predict(test_documents[i],bigram)\n",
    "\t\tpredicted_class_key = inv_label(predicted_class)\n",
    "\t\tif labels[i].split()[0] in label_dict:\n",
    "\t\t\texpected_class_key = labels[i].split()[0]\n",
    "\t\t\tcorrect[indices(expected_class_key)][indices(predicted_class_key)] += 1\n",
    "\treturn (correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'love', u'movi', u'sinc', '7', 'saw', u'open', 'day', u'touch', u'beauti', u'strongli', 'recommend', u'see', u'movi', 'watch', u'famili', 'far', 'mpaa', u'rate', 'pg', '13', u'themat', u'element', u'prolong', u'scene', 'disastor', u'nuditi', u'sexual', u'languag']\n",
      "['first', u'thing', 'first', 'edison', 'chen', u'fantast', u'believ', 'job', 'cambodian', 'hit', 'man', 'born', 'bred', u'dump', u'gladiatori', 'ring', u'hone', 'craft', u'savag', u'batteri', 'order', u'surviv', u'live', 'mantra', 'kill', u'kill', 'role', u'littl', u'dialogu', 'least', u'line', 'cambodian', 'thai', u'perform', u'compel', u'probabl', 'jet', 'li', u'vehicl', u'danni', 'dog', 'man', 'bred', 'sole', u'purpos', u'fight', u'someon', u'els', 'leash', 'like', u'danni', 'dog', 'much', u'talk', 'bare', u'knuckl', 'fight', u'sequenc', u'choreograph', u'stylist', 'rather', u'design', 'normal', 'brutal', u'fisticuff', u'everyth', u'goe', u'probabl', 'brought', u'sens', 'realism', 'grit', 'see', u'charact', 'slug', u'throat', u'defend', u'live', u'take', 'away', u'other', 'grim', u'gritti', 'dark', u'movi', u'liter', u'figur', u'set', 'apart', 'usual', 'run', 'mill', 'cop', 'thriller', u'product', 'edison', u'play', u'hire', 'gun', 'cambodia', u'becom', u'fugit', 'hong', 'kong', 'run', u'cop', 'pickup', 'gone', u'awri', u'lead', 'chase', 'team', 'led', 'cheung', 'siu', 'fai', 'contend', 'maverick', 'member', 'inspector', 'ti', 'sam', 'lee', u'inclus', u'accept', 'team', u'sin', 'father', u'begin', 'cat', u'mous', 'game', 'dark', u'shade', u'shadow', 'seedier', u'look', 'side', 'hong', 'kong', u'stori', u'work', u'multipl', u'level', u'especi', u'charact', u'studi', 'hit', 'man', 'cop', u'opposit', u'side', 'law', 'see', 'within', u'charact', 'black', 'white', u'shade', 'grey', 'hit', 'man', 'see', u'care', 'side', 'got', u'hook', u'develop', u'feel', 'love', 'girl', 'pei', 'pei', u'bring', u'sens', u'matur', u'tender', u'reveal', 'heart', 'gold', 'cop', u'question', u'tactic', u'attitud', u'make', 'wonder', 'one', 'would', u'buckl', u'will', u'anyth', u'take', 'get', 'job', 'done', u'mani', u'interest', u'moment', 'moral', u'question', 'anti', 'hero', u'despic', u'strategi', u'adopt', 'ask', u'make', 'man', u'make', 'beast', u'tendenc', 'switch', u'side', u'depend', u'circumst', 'dark', 'inner', 'streak', 'us', u'transform', 'man', 'dog', 'dog', 'man', 'dog', 'bite', 'dog', u'grip', 'start', 'never', u'let', 'go', 'end', 'though', u'point', 'mid', 'way', u'seem', 'drag', u'especi', 'tender', u'moment', u'suffer', u'know', 'end', 'pick', u'favourit', 'scene', 'must', 'one', 'market', 'food', u'centr', u'extrem', 'well', u'control', u'deliv', u'suspens', u'edg', 'seat', 'moment', 'listen', u'music', 'score', u'dream', 'hear', u'growl', u'dog', u'highli', u'recommend', u'especi', 'think', 'seen', 'almost', u'everyth', 'cop', 'thriller', u'genr']\n",
      "[u'brows', 'discount', 'video', 'bin', u'pick', u'movi', '4', '88', u'fifti', 'percent', 'time', u'movi', 'find', 'bin', 'pure', 'crap', 'mean', u'horribl', 'beyond', 'belief', 'half', 'time', 'turn', u'surprisingli', 'good', u'movi', 'much', 'better', u'expect', 'found', u'engag', 'though', u'obvious', 'made', 'amateur', u'direct', u'noth', 'special', u'stori', u'intrigu', 'good', u'thrill', u'expect', u'comedi', u'disappoint', 'thriller', u'movi', u'surprisingli', 'good', u'natur', u'bloodi', u'violenc', u'profan', u'nuditi', 'sex', u'usual', u'movi', u'requir', 'four', u'element', 'pg', u'rate', 'well', u'deserv', 'like', 'sixteen', u'candl', 'f', 'word', u'use', 'twice', 'brief', u'gratuit', 'nude', 'scene', 'wish', u'romanc', 'corey', 'haim', 'love', 'interest', 'could', u'develop', 'film', 'tend', 'plot', u'heavi', u'potenti', 'good', u'subplot', u'push', 'side', 'instead', u'develop', u'chemistri', 'two', 'end', u'watch', u'careless', 'three', u'minut', u'montag', u'romant', u'endeavor', 'end', u'kiss', 'end', u'littl', u'chemistri', u'seem', u'forc', 'dream', u'machin', 'gem', 'good', 'clean', u'entertain', u'quit', u'forgett', u'especi', 'cast', u'unknown', 'except', 'haim', 'also', 'much', 'better', 'expect', 'score', '7', '10']\n",
      "['gem', 'real', u'piec', 'americana', u'impli', 'self', u'program', 'resist', 'life', u'afirm', u'stori', 'stay', 'away', u'leav', u'pleasur', 'rest', 'us', 'still', u'believ', u'make', u'frost', 'cake', u'truli', u'delect', 'fact', u'base', 'real', u'rag', u'rich', u'stori', 'need', 'nit', 'pick', u'detail', u'chang', 'make', 'compact', u'stori', u'chri', 'cooper', 'one', 'greatest', u'live', u'actor', 'complex', 'self', u'conflict', 'bottom', 'line', 'good', 'core', 'father', u'portray', 'could', u'pull', u'success', u'someon', 'skill', 'insight', u'simpl', u'mind', u'comment', u'refus', 'accept', 'father', u'tri', 'lay', 'law', u'sens', 'may', u'possibl', 'track', u'expos', u'limit', u'comment', u'writer', u'act', u'cynic', u'simpl', u'mind']\n",
      "[u'want', 'se', 'film', u'sinc', 'first', 'time', u'watch', 'trailer', 'deep', u'surpris', 'film', u'element', u'action', u'charact', u'seem', u'littl', '\\xc2', 'cartoonish', 'dark', u'natur', 'film', u'realli', u'make', 'much', u'differ', u'experi', 'instead', 'feel', 'good', u'happi', u'stori', 'film', u'take', u'anoth', u'direct', u'prove', u'uplift', 'also', u'disturb', u'kid', 'understand', 'darker', u'moment', 'film', u'make', 'film', 'rather', u'watchabl', u'adult', 'also', u'impress', u'cinematographi', u'use', u'anim', u'digit', u'anim', u'creat', u'seamless', 'network', u'pan', u'tilt', u'music', 'score', 'solid', u'prove', u'han', 'zimmer', 'go', 'guy', u'come', u'anim', u'score', 'never', 'thought', 'would', 'say', u'actual', u'enjoy', 'brian', u'adam', 'music']\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xc2 in position 0: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-c676a09d6ec1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"imdb_train_text.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# training_data = remove_punct(training_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetStemmedDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtraining_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"imdb_train_labels.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"imdb_test_text.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-165-ed9d7909a6bf>\u001b[0m in \u001b[0;36mgetStemmedDocument\u001b[0;34m(input_raw_data)\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mstemmed_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp_stemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopped_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0;32mprint\u001b[0m \u001b[0mstemmed_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0mdocumentWords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmed_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0mnew_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocumentWords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_doc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xc2 in position 0: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "training_data = read_data(\"imdb_train_text.txt\")\n",
    "# training_data = remove_punct(training_data)\n",
    "training_data = getStemmedDocument(training_data)\n",
    "training_labels = read_data(\"imdb_train_labels.txt\")\n",
    "test_data = read_data(\"imdb_test_text.txt\")\n",
    "# test_data =remove_punct(test_data)\n",
    "test_data = getStemmedDocument(test_data)\n",
    "test_labels = read_data(\"imdb_test_labels.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = make_dict(training_labels,0)\n",
    "vocab_dict = make_dict(training_data,0)\n",
    "print len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_matrix(bigram):\n",
    "\tnaive_matrix = np.ones((len(vocab_dict),number_classes))\n",
    "\tnum_words_in_class = np.full((1,number_classes),len(vocab_dict))\n",
    "\tlabel_freq = np.zeros(number_classes)\n",
    "\tfor i in range(len(training_data)):\n",
    "\t\tlabel = label_dict[training_labels[i].split()[0]]\n",
    "\t\tlabel_freq[label] +=1\n",
    "\t\twords = training_data[i].split()\n",
    "\t\tj = 0\n",
    "\t\tfor word in words:\n",
    "\t\t\tif j>0 and bigram:\n",
    "\t\t\t\tbigram_word = words[j-1] + \" \" + word\n",
    "\t\t\t\tif bigram_word in vocab_dict:\n",
    "\t\t\t\t\tword_index = vocab_dict[bigram_word]\n",
    "\t\t\t\t\tnaive_matrix[word_index][label] +=1\n",
    "\t\t\tword_index = vocab_dict[word]\n",
    "\t\t\tnaive_matrix[word_index][label] +=1\n",
    "\t\t\tnum_words_in_class[0][label] +=1\n",
    "\t\t\tj +=1\n",
    "\treturn (np.log(naive_matrix) - np.log(num_words_in_class)),label_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_matrix,label_freq = make_matrix(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.844\n"
     ]
    }
   ],
   "source": [
    "training_accuracy = accuracy(training_data,training_labels,0)\n",
    "print training_accuracy*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.388\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = accuracy(test_data,test_labels,0)\n",
    "print test_accuracy*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.188\n"
     ]
    }
   ],
   "source": [
    "test_random_accuracy = random_accuracy(test_data,test_labels)\n",
    "print test_random_accuracy*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.088\n"
     ]
    }
   ],
   "source": [
    "test_majority_accuracy = majority_accuracy(test_data,test_labels)\n",
    "print test_majority_accuracy*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.72600000e+03   0.00000000e+00   4.00000000e+00   3.40000000e+01\n",
      "    9.00000000e+00   3.30000000e+01   0.00000000e+00   2.16000000e+02]\n",
      " [  2.02800000e+03   2.00000000e+00   4.00000000e+00   5.60000000e+01\n",
      "    1.30000000e+01   4.40000000e+01   0.00000000e+00   1.55000000e+02]\n",
      " [  1.95800000e+03   1.00000000e+00   2.80000000e+01   1.40000000e+02\n",
      "    2.40000000e+01   1.20000000e+02   0.00000000e+00   2.70000000e+02]\n",
      " [  1.67400000e+03   2.00000000e+00   4.00000000e+00   2.41000000e+02\n",
      "    6.20000000e+01   2.18000000e+02   1.00000000e+00   4.33000000e+02]\n",
      " [  6.56000000e+02   0.00000000e+00   0.00000000e+00   5.50000000e+01\n",
      "    9.10000000e+01   4.36000000e+02   0.00000000e+00   1.06900000e+03]\n",
      " [  6.16000000e+02   0.00000000e+00   2.00000000e+00   3.30000000e+01\n",
      "    3.40000000e+01   4.69000000e+02   2.00000000e+00   1.69400000e+03]\n",
      " [  4.35000000e+02   0.00000000e+00   0.00000000e+00   9.00000000e+00\n",
      "    1.60000000e+01   2.10000000e+02   0.00000000e+00   1.67400000e+03]\n",
      " [  9.75000000e+02   0.00000000e+00   2.00000000e+00   8.00000000e+00\n",
      "    1.30000000e+01   2.10000000e+02   1.00000000e+00   3.79000000e+03]]\n",
      "[[  4.72600000e+03   0.00000000e+00   4.00000000e+00   3.40000000e+01\n",
      "    9.00000000e+00   3.30000000e+01   0.00000000e+00   2.16000000e+02]\n",
      " [  2.02800000e+03   2.00000000e+00   4.00000000e+00   5.60000000e+01\n",
      "    1.30000000e+01   4.40000000e+01   0.00000000e+00   1.55000000e+02]\n",
      " [  1.95800000e+03   1.00000000e+00   2.80000000e+01   1.40000000e+02\n",
      "    2.40000000e+01   1.20000000e+02   0.00000000e+00   2.70000000e+02]\n",
      " [  1.67400000e+03   2.00000000e+00   4.00000000e+00   2.41000000e+02\n",
      "    6.20000000e+01   2.18000000e+02   1.00000000e+00   4.33000000e+02]\n",
      " [  6.56000000e+02   0.00000000e+00   0.00000000e+00   5.50000000e+01\n",
      "    9.10000000e+01   4.36000000e+02   0.00000000e+00   1.06900000e+03]\n",
      " [  6.16000000e+02   0.00000000e+00   2.00000000e+00   3.30000000e+01\n",
      "    3.40000000e+01   4.69000000e+02   2.00000000e+00   1.69400000e+03]\n",
      " [  4.35000000e+02   0.00000000e+00   0.00000000e+00   9.00000000e+00\n",
      "    1.60000000e+01   2.10000000e+02   0.00000000e+00   1.67400000e+03]\n",
      " [  9.75000000e+02   0.00000000e+00   2.00000000e+00   8.00000000e+00\n",
      "    1.30000000e+01   2.10000000e+02   1.00000000e+00   3.79000000e+03]]\n"
     ]
    }
   ],
   "source": [
    "confuse = confusion_matrix(test_data,test_labels,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4726 0 4 34 9 33 0 216\n",
      "2028 2 4 56 13 44 0 155\n",
      "1958 1 28 140 24 120 0 270\n",
      "1674 2 4 241 62 218 1 433\n",
      "656 0 0 55 91 436 0 1069\n",
      "616 0 2 33 34 469 2 1694\n",
      "435 0 0 9 16 210 0 1674\n",
      "975 0 2 8 13 210 1 3790\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(confuse)):\n",
    "    for j in range(len(confuse[0])):\n",
    "        print (int)(confuse[i][j]),\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10': 0, '1': 7, '3': 4, '2': 6, '4': 5, '7': 2, '9': 3, '8': 1}\n"
     ]
    }
   ],
   "source": [
    "print label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
