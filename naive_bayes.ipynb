{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "import sys\n",
    "\n",
    "\n",
    "#hyper_parameter\n",
    "C = 1\n",
    "bigram_thresh = 7\n",
    "negation_thresh = 2\n",
    "punct_list = [\",\",\".\",\"/\",\"\\\"\"]\n",
    "negate_list = [\"not\",\"no\",\"never\",\"didn't\",\"nt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/saket/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/saket/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/saket/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initializing stemmer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "en_stop = set(stopwords.words('english'))\n",
    "p_stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(documents):\n",
    "\tnew_documents = []\n",
    "\tfor document in documents:\n",
    "\t\tfor i in range(len(punct_list)):\n",
    "\t\t\tdocument = document.lower()\n",
    "\t\t\tdocument = document.replace(punct_list[i],\" \")\n",
    "\t\tnew_documents.append(document)\n",
    "\treturn new_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStemmedDocument(input_raw_data,adjective,negation_token):\n",
    "    docs = input_raw_data\n",
    "    new_doc = []\n",
    "    for doc in docs:\n",
    "        doc = doc.decode('utf-8')        \n",
    "        raw = doc.lower()\n",
    "        raw = raw.replace(\"<br /><br />\", \" \")\n",
    "        raw.replace(\" br \",\" \")\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        if adjective>0 :\n",
    "            pos = nltk.pos_tag(tokens)\n",
    "            adj_list = [tag[0] for tag in pos if tag[1] == 'JJ']\n",
    "        stopped_tokens = [token for token in tokens if token not in en_stop]\n",
    "        stemmed_tokens = [p_stemmer.stem(token) for token in stopped_tokens]\n",
    "        if adjective>0 :\n",
    "            stemmed_adj_tokens = [p_stemmer.stem(token) for token in adj_list]\n",
    "            for i in range(adjective):\n",
    "                stemmed_tokens = stemmed_tokens + stemmed_adj_tokens\n",
    "        if negation_token:\n",
    "            stemmed_tokens = not_clear(stemmed_tokens)\n",
    "        documentWords = ' '.join(stemmed_tokens)\n",
    "        new_doc.append(documentWords)\n",
    "    return new_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_clear(tokens):\n",
    "    i =0\n",
    "    for token in tokens:\n",
    "        if token in negate_list:\n",
    "            if i+1 < len(tokens):\n",
    "                tokens[i+1] = \"not\" + tokens[i+1]\n",
    "            if i+2 < len(tokens):\n",
    "                tokens[i+2] = \"not\" + tokens[i+2]\n",
    "        i+=1\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "\tfile = open(\"data/\" + file_name)\n",
    "\tall_text = file.readlines()\n",
    "\tif len(all_text)==0:\n",
    "\t\tprint \"empty document\"\n",
    "\treturn all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dict(training_data,bigram):\n",
    "\tvocab_dict = {}\n",
    "\tbigram_dict = {}\n",
    "\tcount = 0   \n",
    "\tfor document in training_data:\n",
    "\t\twords = document.split()\n",
    "\t\ti = 0\n",
    "\t\tfor word in words:\n",
    "\t\t\tif i > 0 and bigram:\n",
    "\t\t\t\tbigram_word = words[i-1] + \" \" + word\n",
    "\t\t\t\tif not bigram_word in bigram_dict:\n",
    "\t\t\t\t\tbigram_dict[bigram_word] = 1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tbigram_dict[bigram_word] +=1\n",
    "\t\t\tif not word in vocab_dict:\n",
    "\t\t\t\tvocab_dict[word] = count\n",
    "\t\t\t\tcount += 1\n",
    "\t\t\ti +=1\n",
    "\tif bigram:\n",
    "\t\tfor key in bigram_dict.keys():\n",
    "\t\t\tif bigram_dict[key] >= bigram_thresh:              \n",
    "\t\t\t\tvocab_dict[key] = count\n",
    "\t\t\t\tcount += 1\n",
    "\treturn vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_document,bigram):\n",
    "\tglobal naive_matrix,label_freq,number_classes,vocab_dict\n",
    "\tmax_sum = 0\n",
    "\tpredicted_class = -1\n",
    "\tfor class_ in range(number_classes):\n",
    "\t\tsums = math.log(label_freq[class_])\n",
    "\t\twords = test_document.split()\n",
    "\t\tj = 0\n",
    "\t\tfor word in words:\n",
    "\t\t\tif j>0 and bigram:\n",
    "\t\t\t\tbigram_word = words[j-1] + \" \" + word\n",
    "\t\t\t\tif bigram_word in vocab_dict:\n",
    "\t\t\t\t\tword_index = vocab_dict[bigram_word]\n",
    "\t\t\t\t\tsums += naive_matrix[word_index][class_]\n",
    "\t\t\tif word in vocab_dict:\n",
    "\t\t\t\tword_index = vocab_dict[word]\n",
    "\t\t\t\tsums += naive_matrix[word_index][class_]\n",
    "\t\t\tj += 1\n",
    "\t\tif sums > max_sum or class_==0:\n",
    "\t\t\tmax_sum = sums\n",
    "\t\t\tpredicted_class = class_\n",
    "\treturn predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_label(label_val):\n",
    "    for key in label_dict.keys():\n",
    "        if label_dict[key] == label_val:\n",
    "            return key\n",
    "def indices(label_key):\n",
    "    label_key = int(label_key)\n",
    "    if label_key >= 7:\n",
    "        label_key -= 2\n",
    "    return label_key - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(test_documents,labels,bigram):\n",
    "\tcorrect = 0.0\n",
    "\tfor i in range(len(test_documents)):\n",
    "\t\tpredicted_class = predict(test_documents[i],bigram)\n",
    "\t\tif labels[i].split()[0] in label_dict.keys():\n",
    "\t\t\texpected_class = label_dict[labels[i].split()[0]]\n",
    "\t\t\tif predicted_class==expected_class:\n",
    "\t\t\t\tcorrect+=1\n",
    "\treturn correct/len(test_documents)\n",
    "\n",
    "def majority_accuracy(test_documents,labels):\n",
    "\tcorrect = 0.0\n",
    "\tfor i in range(len(test_documents)):\n",
    "\t\tpredicted_class = np.argmax(label_freq)\n",
    "\t\tif labels[i].split()[0] in label_dict.keys():\n",
    "\t\t\texpected_class = label_dict[labels[i].split()[0]]\n",
    "\t\t\tif predicted_class==expected_class:\n",
    "\t\t\t\tcorrect+=1\n",
    "\treturn correct/len(test_documents)\n",
    "\n",
    "def random_accuracy(test_documents,labels):\n",
    "\tcorrect = 0.0\n",
    "\tfor i in range(len(test_documents)):\n",
    "\t\tpredicted_class = random.randint(0,len(label_dict))\n",
    "\t\tif labels[i].split()[0] in label_dict:\n",
    "\t\t\texpected_class = label_dict[labels[i].split()[0]]\n",
    "\t\t\tif predicted_class==expected_class:\n",
    "\t\t\t\tcorrect+=1\n",
    "\treturn correct/len(test_documents)\n",
    "\n",
    "def confusion_matrix(test_documents,labels,bigram):\n",
    "\tcorrect = np.zeros((len(label_dict),len(label_dict)))\n",
    "\tfor i in range(len(test_documents)):\n",
    "\t\tpredicted_class = predict(test_documents[i],bigram)\n",
    "\t\tpredicted_class_key = inv_label(predicted_class)\n",
    "\t\tif labels[i].split()[0] in label_dict:\n",
    "\t\t\texpected_class_key = labels[i].split()[0]\n",
    "\t\t\tcorrect[indices(expected_class_key)][indices(predicted_class_key)] += 1\n",
    "\treturn (correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf_fill(training_data):\n",
    "    idf_count = np.zeros(len(vocab_dict))\n",
    "    for doc in training_data:\n",
    "        words = doc.split()\n",
    "        word_set = set()\n",
    "        for word in words:\n",
    "            word_set.add(word)\n",
    "        for word in word_set:\n",
    "            word_index = vocab_dict[word]\n",
    "            idf_count[word_index] += 1\n",
    "    idf_count =  np.log(len(training_data)) - np.log(idf_count)\n",
    "    return idf_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_matrix(bigram,idf):\n",
    "    naive_matrix = np.ones((len(vocab_dict),number_classes))\n",
    "    naive_matrix = C * naive_matrix    \n",
    "    num_words_in_class = np.full((1,number_classes),C*len(vocab_dict))\n",
    "    label_freq = np.zeros(number_classes)\n",
    "    for i in range(len(training_data)):\n",
    "        label = label_dict[training_labels[i].split()[0]]\n",
    "        label_freq[label] +=1\n",
    "        words = training_data[i].split()\n",
    "        j = 0\n",
    "        for word in words:\n",
    "            if j>0 and bigram:\n",
    "                bigram_word = words[j-1] + \" \" + word\n",
    "                if bigram_word in vocab_dict:\n",
    "                    word_index = vocab_dict[bigram_word]\n",
    "                    naive_matrix[word_index][label] +=1\n",
    "            word_index = vocab_dict[word]\n",
    "            naive_matrix[word_index][label] += (1 + idf*idf_count[word_index])\n",
    "            num_words_in_class[0][label] += (1 + idf*idf_count[word_index])\n",
    "            j +=1\n",
    "    return (np.log(naive_matrix) - np.log(num_words_in_class)),label_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.144\n",
      "41.3\n",
      "4552 28 68 132 21 47 8 166\n",
      "1769 36 106 210 30 35 1 115\n",
      "1556 29 202 403 71 105 4 171\n",
      "1175 14 182 647 178 187 12 240\n",
      "384 5 63 223 416 450 33 733\n",
      "376 7 45 154 238 635 54 1341\n",
      "294 3 16 76 121 362 55 1417\n",
      "571 8 23 75 107 374 59 3782\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "feature = 3\n",
    "training_data = read_data(\"imdb_train_text.txt\")\n",
    "if feature==2 or feature ==3 or feature ==5:\n",
    "    training_data = getStemmedDocument(training_data,0,0)\n",
    "elif feature==4:\n",
    "    training_data = getStemmedDocument(training_data,0,1)\n",
    "training_labels = read_data(\"imdb_train_labels.txt\")\n",
    "test_data = read_data(\"imdb_test_text.txt\")\n",
    "if feature==2 or feature ==3 or feature ==5:\n",
    "    test_data = getStemmedDocument(test_data,0,0)\n",
    "elif feature ==4:\n",
    "    test_data = getStemmedDocument(test_data,0,1)\n",
    "test_labels = read_data(\"imdb_test_labels.txt\")\n",
    "if feature == 3:\n",
    "    bigram = 1\n",
    "else:\n",
    "    bigram = 0\n",
    "label_dict = make_dict(training_labels,0)\n",
    "vocab_dict = make_dict(training_data,bigram)    \n",
    "number_classes = len(label_dict)\n",
    "idf_count = np.zeros(len(vocab_dict))\n",
    "if feature ==5:\n",
    "    idf_count  = idf_fill(training_data)\n",
    "    naive_matrix,label_freq = make_matrix(bigram,1)\n",
    "else:\n",
    "    naive_matrix,label_freq = make_matrix(bigram,0)\n",
    "import pickle\n",
    "with open('./pickles/naive_model3.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab_dict,f)\n",
    "    pickle.dump(label_dict,f)\n",
    "    pickle.dump(naive_matrix,f)\n",
    "    pickle.dump(label_freq,f)\n",
    "training_accuracy = accuracy(training_data,training_labels,bigram)\n",
    "print training_accuracy*100\n",
    "test_accuracy = accuracy(test_data,test_labels,bigram)\n",
    "print test_accuracy*100\n",
    "confuse = confusion_matrix(test_data,test_labels,bigram)\n",
    "for i in range(len(confuse)):\n",
    "    for j in range(len(confuse[0])):\n",
    "        print (int)(confuse[i][j]),\n",
    "    print\n",
    "print \"================================================================================\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "make_matrix() takes exactly 2 arguments (4 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-c9460b66e929>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"normal\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfeature_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"stemmed\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfeature_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"bigram\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-74-db72defde957>\u001b[0m in \u001b[0;36mfeature_selection\u001b[0;34m(feature)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mnaive_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mnaive_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mnaive_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mtraining_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnaive_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: make_matrix() takes exactly 2 arguments (4 given)"
     ]
    }
   ],
   "source": [
    "# print \"normal\"\n",
    "# feature_selection(1)\n",
    "# print \"stemmed\"\n",
    "# feature_selection(2)\n",
    "# print \"bigram\"\n",
    "# feature_selection(3)\n",
    "# print \"negation\"\n",
    "# feature_selection(4)\n",
    "# print \"idf\"\n",
    "# feature_selection(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
